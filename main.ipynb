{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "\n",
    "from training.llama_trainer import LlamaTrainer\n",
    "from training.config import TrainingConfig\n",
    "from llama.config import LlamaConfig\n",
    "from llama.llama3 import Llama3\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "llama_config = LlamaConfig(\n",
    "    vocab_size=32000,        # Reduced from 50257\n",
    "    context_length=1024,     # Keep as is\n",
    "    embedding_dim=512,       # Reduced from 624\n",
    "    num_heads=8,            # Reduced from 12\n",
    "    num_layers=8,           # Reduced from 10\n",
    "    hidden_dim=2048,        # Reduced from 3072\n",
    "    num_kv_groups=4,        # Keep as is\n",
    "    rope_base=10000,        # Keep as is\n",
    "    dtype=torch.bfloat16    # Keep as is\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=1e-4,      # Reduced from 1e-3\n",
    "    max_learning_rate=3e-4,  # Reduced from 5e-4\n",
    "    num_epochs=3,           # Reduced from 10\n",
    "    patience=2,             # Reduced from 3\n",
    "    checkpoint_path=Path('checkpoints/llama_80m.pth'),\n",
    "    dataset_path=\"data/pes2o\",\n",
    "    batch_size=16,          # Reduced from 32\n",
    "    context_length=1024     # Keep as is\n",
    ")\n",
    "\n",
    "model = Llama3(llama_config).to(device)\n",
    "trainer = LlamaTrainer(model, training_config, device)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
